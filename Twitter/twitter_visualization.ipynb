{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "credentials = pd.read_csv('credentials.csv')\n",
    "consumer_key = credentials['consumer_key'][0]\n",
    "consumer_secret = credentials['consumer_secret'][0]\n",
    "access_token = credentials['access_token'][0]\n",
    "access_secret = credentials['access_secret'][0]\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "RT @LiveGreenTO: Do you live in an apartment, condo or co-op in Toronto? Make a difference by helping to reduce waste in your building by eâ€¦\n",
      "Awe the fact that Americans are getting the hashtag #ThanksCanada to trend is making my heart melt.\n",
      "\n",
      "Thanks all. https://t.co/TDWM0LxE6s\n",
      "RT @TwitterBusiness: #How #many #hashtags #is #too #many?\n",
      "\n",
      "https://t.co/xGcgpoHuQC\n",
      "RT @TorontoComms: Green bins are now being piloted in 20 #TOparks Off-Leash Dog Areas to reduce the amount of organic waste in public Blueâ€¦\n",
      "Aweeeeee! https://t.co/QWcTun37hB\n",
      "#NoPooinBlue https://t.co/jELSoiZjX8\n",
      "RT @TOAnimalService: Here's Potter with an important message: put dog poop in Green Bins in #TOparks. The City is piloting the use of Greenâ€¦\n",
      "Do you remember when you joined Twitter? I do! #MyTwitterAnniversary https://t.co/36KOB4jJN1\n",
      "RT @TorontoComms: Unclear about what to do with clear plastic food containers? You can recycle them. Rinse clean and put them loose in theâ€¦\n"
     ]
    }
   ],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_tweets = api.user_timeline(screen_name = \"amnasri2\",count=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_api',\n",
       " '_json',\n",
       " 'author',\n",
       " 'contributors',\n",
       " 'coordinates',\n",
       " 'created_at',\n",
       " 'destroy',\n",
       " 'entities',\n",
       " 'favorite',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'parse',\n",
       " 'parse_list',\n",
       " 'place',\n",
       " 'retweet',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'retweets',\n",
       " 'source',\n",
       " 'source_url',\n",
       " 'text',\n",
       " 'truncated',\n",
       " 'user']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(my_tweets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaster Words:\n",
    "\n",
    "Obtain disaster words from emdat website: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "r  = requests.get(\"https://www.emdat.be/classification\", verify=False)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, \"html\")\n",
    "tbody_list = soup.find_all('tbody')\n",
    "\n",
    "td_list = []\n",
    "for tbody in tbody_list:\n",
    "    td_list.extend(tbody.find_all('td'))\n",
    "\n",
    "disaster_words = [td.text.lower() for td in td_list[6:-9] if 1<=len(td.text.split()) <=3]\n",
    "\n",
    "disaster_words = [s.replace('/', ' ').strip() for s in disaster_words]\n",
    "\n",
    "\n",
    "import re\n",
    "disaster_words = set(re.sub('\\s+', ' ', s) for s in disaster_words) - set([u'disaster group',\n",
    " u'disaster subgroup',\n",
    " u'disaster main type',\n",
    " u'disaster sub-type',\n",
    " u'disaster sub-sub-type',])\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#porter_stemmer = PorterStemmer()\n",
    "#set([wordnet_lemmatizer.lemmatize(wordnet_lemmatizer.lemmatize(w, pos='v')) for w in disaster_words]) - disaster_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tweets from tweeter stream from a given location (bounding rectangle) that contain disaster words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import numpy as np\n",
    "\n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        global cnt\n",
    "        try:\n",
    "            #print('data: %s' % data)\n",
    "            tweet = json.loads(data)\n",
    "            '''\n",
    "            geo = tweet['geo']\n",
    "            user_location = tweet['user']['location'] \n",
    "            city = None\n",
    "            state = None\n",
    "            \n",
    "            try:\n",
    "                second_part = user_location.split(',')[1].lower()\n",
    "                if second_part in states_abbr:\n",
    "                    state = second_part\n",
    "                elif second_part in ['us', 'usa', 'canada']:\n",
    "                    country = second_part\n",
    "                city = user_location.split(',')[0].lower()\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            if not geo and not city:\n",
    "                return\n",
    "                    \n",
    "            text = tweet['text']\n",
    "            #print(geo, \"%s, %s\" % (city, state), tweet['coordinates'], tweet['place'])\n",
    "            print(tweet['text'])\n",
    "            print\n",
    "            print(tweet['geo'])\n",
    "            print\n",
    "            print(tweet['coordinates'])\n",
    "            print\n",
    "            '''\n",
    "            coordinates = np.array(tweet['place']['bounding_box']['coordinates'][0]).mean(axis=0)\n",
    "            #print(coordinates)\n",
    "            #print(\"-----------------------------------------------------\\n\\n\")\n",
    "            \"\"\"\n",
    "            cnt = 0\n",
    "            words = [w.strip().lower() for w in text.split()]\n",
    "            for w in words:\n",
    "                if w in disaster_words:\n",
    "                    cnt += 1\n",
    "            if cnt != 0:\n",
    "                print(text)\n",
    "                print(\"%s\\n\" % cnt)\n",
    "            \"\"\"\n",
    "            \n",
    "            f_coordinates.write(\"%s,%s\\n\" % tuple(coordinates.tolist()))\n",
    "\n",
    "            f_tweets.write(tweet['text'].encode('utf-8'))\n",
    "            tweets.append(tweet['text'].encode('utf-8'))\n",
    "                \n",
    "            cnt += 1\n",
    "            if cnt % 100 == 0:\n",
    "                print(cnt)\n",
    "            if cnt == max_count:\n",
    "                twitter_stream.disconnect()\n",
    "                print(\"done!\")\n",
    "                f_coordinates.close()\n",
    "                f_tweets.close()\n",
    "                \n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "f_coordinates = open('coordinates', 'w')\n",
    "f_tweets = open('tweets', 'w')\n",
    "tweets = []\n",
    "cnt = 0\n",
    "max_count = 1000\n",
    "twitter_stream = Stream(auth=auth, listener=MyListener())\n",
    "#twitter_stream.filter(track=['a'], async=True)\n",
    "#GEOBOX_GERMANY = [5.0770049095, 47.2982950435, 15.0403900146, 54.9039819757]\n",
    "GEOBOX_US_CANADA = [-128.755117, 26.415893, -52.437305, 54.093165]\n",
    "#twitter_stream.filter(locations=GEOBOX_GERMANY, async=True)\n",
    "#twitter_stream.filter(locations=[-6.38,49.87,1.77,55.81], async=True)\n",
    "twitter_stream.filter(locations=GEOBOX_US_CANADA, async=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_stream.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-83.030573,42.581306\n",
      "-111.7375655,33.4057545\n",
      "-95.770195,36.0393055\n",
      "-122.3154215,37.552481\n",
      "-98.152419,26.3475255\n",
      "-74.153947,40.572376\n",
      "-87.732013,41.8335845\n",
      "-83.8401315,36.091236\n",
      "-96.7617535,32.8198585\n",
      "-118.080024400\n"
     ]
    }
   ],
   "source": [
    "!tail coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMAOOOOO https://t.co/LAXEXIvYil\n",
      "\n",
      "Damn Ariannas baptism is next week already !!!!!\n",
      "\n",
      "https://t.co/oAdnY6sKBr\n",
      "\n",
      "Esque no les da HUEVA vivir!!!!!!?????? Netaaaaaaa\n",
      "\n",
      "Sometimes u just roll something so beautiful ðŸ˜¢\n",
      "\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "!tail python.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you know the 9 letter encryption on the yellow frog sticker, you get to choose the next encrypted message on the next sticker | Joplin, MO\n",
      "\n",
      "\n",
      "it was a blessing to do both https://t.co/dsHCHboYqy | Washington, DC\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "api = tweepy.API(auth)\n",
    "places = api.geo_search(query=\"USA\", granularity=\"country\", max_results=20)\n",
    "place_id = places[0].id\n",
    "\n",
    "tweets = api.search(q=\"place:%s\" % place_id)\n",
    "for tweet in tweets:\n",
    "    print tweet.text + \" | \" + (tweet.place.full_name if tweet.place else \"Undefined place\") + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gmplot\n",
    "output_html = \"my_map.html\"\n",
    "apikey = \"AIzaSyAaN6JdzBYDQuN_P8U3xjAsg4V4GwSvB6Y\"\n",
    "center_long = (GEOBOX_US_CANADA[0] + GEOBOX_US_CANADA[2])/2\n",
    "center_lat = (GEOBOX_US_CANADA[1] + GEOBOX_US_CANADA[3])/2\n",
    "gmap = gmplot.GoogleMapPlotter(center_lat, center_long, 4, apikey=apikey)\n",
    "# Polygon\n",
    "golden_gate_park_lats, golden_gate_park_lons = zip(*[\n",
    "    (37.771269, -122.511015),\n",
    "    (37.773495, -122.464830),\n",
    "    (37.774797, -122.454538),\n",
    "    (37.771988, -122.454018),\n",
    "    (37.773646, -122.440979),\n",
    "    (37.772742, -122.440797),\n",
    "    (37.771096, -122.453889),\n",
    "    (37.768669, -122.453518),\n",
    "    (37.766227, -122.460213),\n",
    "    (37.764028, -122.510347),\n",
    "    (37.771269, -122.511015)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "#gmap.plot(golden_gate_park_lats, golden_gate_park_lons, 'cornflowerblue', edge_width=10)\n",
    "#gmap.heatmap(golden_gate_park_lats, golden_gate_park_lons, radius=30)\n",
    "\n",
    "\n",
    "locations_longs, locations_lats = zip(*[[-97.662618, 27.578509], \n",
    "                                        [-97.662618, 27.895793], \n",
    "                                        [-97.202232, 27.895793], \n",
    "                                        [-97.202232, 27.578509]])\n",
    "\n",
    "locations_longs, locations_lats = zip(*pd.read_csv('coordinates', header=None).values.tolist())\n",
    "\n",
    "#gmap.plot(locations_lats, locations_longs, 'cornflowerblue', edge_width=10)\n",
    "gmap.heatmap(locations_lats, locations_longs, radius=15)\n",
    "\n",
    "\"\"\"\n",
    "# Scatter points\n",
    "top_attraction_lats, top_attraction_lons = zip(*[\n",
    "    (37.769901, -122.498331),\n",
    "    (37.768645, -122.475328),\n",
    "    (37.771478, -122.468677),\n",
    "    (37.769867, -122.466102),\n",
    "    (37.767187, -122.467496),\n",
    "    (37.770104, -122.470436)\n",
    "    ])\n",
    "gmap.scatter(top_attraction_lats, top_attraction_lons, '#3B0B39', size=40, marker=False)\n",
    "\n",
    "# Marker\n",
    "hidden_gem_lat, hidden_gem_lon = 37.770776, -122.461689\n",
    "gmap.marker(hidden_gem_lat, hidden_gem_lon, 'cornflowerblue')\n",
    "\"\"\"\n",
    "\n",
    "# Draw\n",
    "gmap.draw(output_html)\n",
    "\n",
    "import os\n",
    "os.system(\"cp %s /var/www/html\" % output_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head>\n",
       "<meta name=\"viewport\" content=\"initial-scale=1.0, user-scalable=no\" />\n",
       "<meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\"/>\n",
       "<title>Google Maps - pygmaps </title>\n",
       "<script type=\"text/javascript\" src=\"https://maps.googleapis.com/maps/api/js?libraries=visualization&sensor=true_or_false&key=AIzaSyAaN6JdzBYDQuN_P8U3xjAsg4V4GwSvB6Y\"></script>\n",
       "<script type=\"text/javascript\">\n",
       "\tfunction initialize() {\n",
       "\t\tvar centerlatlng = new google.maps.LatLng(37.766956, -122.438481);\n",
       "\t\tvar myOptions = {\n",
       "\t\t\tzoom: 13,\n",
       "\t\t\tcenter: centerlatlng,\n",
       "\t\t\tmapTypeId: google.maps.MapTypeId.ROADMAP\n",
       "\t\t};\n",
       "\t\tvar map = new google.maps.Map(document.getElementById(\"map_canvas\"), myOptions);\n",
       "\n",
       "var heatmap_points = [\n",
       "new google.maps.LatLng(7.993561, 53.501976),\n",
       "new google.maps.LatLng(7.993561, 53.637865),\n",
       "new google.maps.LatLng(8.171061, 53.637865),\n",
       "new google.maps.LatLng(8.171061, 53.501976),\n",
       "];\n",
       "\n",
       "var pointArray = new google.maps.MVCArray(heatmap_points);\n",
       "var heatmap;\n",
       "heatmap = new google.maps.visualization.HeatmapLayer({\n",
       "\n",
       "data: pointArray\n",
       "});\n",
       "heatmap.setMap(map);\n",
       "heatmap.set('threshold', 10);\n",
       "heatmap.set('radius', 30);\n",
       "heatmap.set('opacity', 0.600000);\n",
       "heatmap.set('dissipating', true);\n",
       "\t}\n",
       "</script>\n",
       "</head>\n",
       "<body style=\"margin:0px; padding:0px;\" onload=\"initialize()\">\n",
       "\t<div id=\"map_canvas\" style=\"width: 100%; height: 100%;\"></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "f = open(output_html)\n",
    "s = f.read()\n",
    "display(HTML('<html>\\n<head>\\n<meta name=\"viewport\" content=\"initial-scale=1.0, user-scalable=no\" />\\n<meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\"/>\\n<title>Google Maps - pygmaps </title>\\n<script type=\"text/javascript\" src=\"https://maps.googleapis.com/maps/api/js?libraries=visualization&sensor=true_or_false&key=AIzaSyAaN6JdzBYDQuN_P8U3xjAsg4V4GwSvB6Y\"></script>\\n<script type=\"text/javascript\">\\n\\tfunction initialize() {\\n\\t\\tvar centerlatlng = new google.maps.LatLng(37.766956, -122.438481);\\n\\t\\tvar myOptions = {\\n\\t\\t\\tzoom: 13,\\n\\t\\t\\tcenter: centerlatlng,\\n\\t\\t\\tmapTypeId: google.maps.MapTypeId.ROADMAP\\n\\t\\t};\\n\\t\\tvar map = new google.maps.Map(document.getElementById(\"map_canvas\"), myOptions);\\n\\nvar heatmap_points = [\\nnew google.maps.LatLng(7.993561, 53.501976),\\nnew google.maps.LatLng(7.993561, 53.637865),\\nnew google.maps.LatLng(8.171061, 53.637865),\\nnew google.maps.LatLng(8.171061, 53.501976),\\n];\\n\\nvar pointArray = new google.maps.MVCArray(heatmap_points);\\nvar heatmap;\\nheatmap = new google.maps.visualization.HeatmapLayer({\\n\\ndata: pointArray\\n});\\nheatmap.setMap(map);\\nheatmap.set(\\'threshold\\', 10);\\nheatmap.set(\\'radius\\', 30);\\nheatmap.set(\\'opacity\\', 0.600000);\\nheatmap.set(\\'dissipating\\', true);\\n\\t}\\n</script>\\n</head>\\n<body style=\"margin:0px; padding:0px;\" onload=\"initialize()\">\\n\\t<div id=\"map_canvas\" style=\"width: 100%; height: 100%;\"></div>\\n</body>\\n</html>\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.compile(r\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('states_abbr.txt')\n",
    "states_abbr = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  127  9233 69508 tweets\r\n"
     ]
    }
   ],
   "source": [
    "!wc tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/amir/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-b83819022fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'headers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'footers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quotes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-168-b83819022fa8>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOPWORDS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatize_stemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-168-b83819022fa8>\u001b[0m in \u001b[0;36mlemmatize_stemming\u001b[0;34m(text, stem)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_stemming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/stem/wordnet.pyc\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/amir/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text, stem=True):\n",
    "    result = lemmatizer.lemmatize(text, pos='v')\n",
    "    if stem:\n",
    "        result = stemmer.stem(result)\n",
    "    return result\n",
    "    \n",
    "def preprocess(text):\n",
    "    \n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and len(token) < 40:\n",
    "            result.append(lemmatize_stemming(token, stem=False))\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = [preprocess(d) for d in dataset.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/lib-tk/Tkinter.py\", line 1489, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 1577, in _download\n",
      "    return self._download_threaded(*e)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 1840, in _download_threaded\n",
      "    assert self._download_msg_queue == []\n",
      "AssertionError\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/lib-tk/Tkinter.py\", line 1489, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 1577, in _download\n",
      "    return self._download_threaded(*e)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/nltk/downloader.py\", line 1840, in _download_threaded\n",
      "    assert self._download_msg_queue == []\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-9ba2cb6b436d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# NMF is able to use tf-idf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mtfidf_feature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# Data cleaning\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 5\n",
    "\n",
    "# Run NMF\n",
    "#nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=15, learning_method='online', learning_decay=.9, learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "#display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
