{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "credentials = pd.read_csv('credentials.csv')\n",
    "consumer_key = credentials['consumer_key'][0]\n",
    "consumer_secret = credentials['consumer_secret'][0]\n",
    "access_token = credentials['access_token'][0]\n",
    "access_secret = credentials['access_secret'][0]\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Simpsons_tweets: https://t.co/vg9Xo0pzqb\n",
      "Love #HeritageMinutes.  This is a great one!  Not crying at all.... https://t.co/Y4UJVstgTg\n",
      "test\n",
      "RT @LiveGreenTO: Do you live in an apartment, condo or co-op in Toronto? Make a difference by helping to reduce waste in your building by e…\n",
      "Awe the fact that Americans are getting the hashtag #ThanksCanada to trend is making my heart melt.\n",
      "\n",
      "Thanks all. https://t.co/TDWM0LxE6s\n",
      "RT @TwitterBusiness: #How #many #hashtags #is #too #many?\n",
      "\n",
      "https://t.co/xGcgpoHuQC\n",
      "RT @TorontoComms: Green bins are now being piloted in 20 #TOparks Off-Leash Dog Areas to reduce the amount of organic waste in public Blue…\n",
      "Aweeeeee! https://t.co/QWcTun37hB\n",
      "#NoPooinBlue https://t.co/jELSoiZjX8\n",
      "RT @TOAnimalService: Here's Potter with an important message: put dog poop in Green Bins in #TOparks. The City is piloting the use of Green…\n"
     ]
    }
   ],
   "source": [
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    print(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_tweets = api.user_timeline(screen_name = \"amnasri2\",count=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_api',\n",
       " '_json',\n",
       " 'author',\n",
       " 'contributors',\n",
       " 'coordinates',\n",
       " 'created_at',\n",
       " 'destroy',\n",
       " 'entities',\n",
       " 'favorite',\n",
       " 'favorite_count',\n",
       " 'favorited',\n",
       " 'geo',\n",
       " 'id',\n",
       " 'id_str',\n",
       " 'in_reply_to_screen_name',\n",
       " 'in_reply_to_status_id',\n",
       " 'in_reply_to_status_id_str',\n",
       " 'in_reply_to_user_id',\n",
       " 'in_reply_to_user_id_str',\n",
       " 'is_quote_status',\n",
       " 'lang',\n",
       " 'parse',\n",
       " 'parse_list',\n",
       " 'place',\n",
       " 'retweet',\n",
       " 'retweet_count',\n",
       " 'retweeted',\n",
       " 'retweeted_status',\n",
       " 'retweets',\n",
       " 'source',\n",
       " 'source_url',\n",
       " 'text',\n",
       " 'truncated',\n",
       " 'user']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(my_tweets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disaster Words:\n",
    "\n",
    "Obtain disaster words from emdat website: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "r  = requests.get(\"https://www.emdat.be/classification\", verify=False)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, \"html\")\n",
    "tbody_list = soup.find_all('tbody')\n",
    "\n",
    "td_list = []\n",
    "for tbody in tbody_list:\n",
    "    td_list.extend(tbody.find_all('td'))\n",
    "\n",
    "disaster_words = [td.text.lower() for td in td_list[6:-9] if 1<=len(td.text.split()) <=3]\n",
    "\n",
    "disaster_words = [s.replace('/', ' ').strip() for s in disaster_words]\n",
    "\n",
    "\n",
    "import re\n",
    "disaster_words = set(re.sub('\\s+', ' ', s) for s in disaster_words) - set([u'disaster group',\n",
    " u'disaster subgroup',\n",
    " u'disaster main type',\n",
    " u'disaster sub-type',\n",
    " u'disaster sub-sub-type',])\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#porter_stemmer = PorterStemmer()\n",
    "#set([wordnet_lemmatizer.lemmatize(wordnet_lemmatizer.lemmatize(w, pos='v')) for w in disaster_words]) - disaster_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tweets from tweeter stream from a given location (bounding rectangle) that contain disaster words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ea583f6380d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_db = client.tweets_db.tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.drop_database('db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loc': [-73.9685415, 40.780709],\n",
       " 'text': 'Was that @rmlimodriver69 I just seen at the #HallandOates #Train concert? #69 #68'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "DuplicateKeyError",
     "evalue": "E11000 duplicate key error collection: tweets_db.tweets index: _id_ dup key: { : ObjectId('5b22fb2e8708dc2fe3c3124b') }",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateKeyError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-9c77c071ee7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/collection.pyc\u001b[0m in \u001b[0;36minsert_one\u001b[0;34m(self, document, bypass_document_validation, session)\u001b[0m\n\u001b[1;32m    681\u001b[0m             self._insert(document,\n\u001b[1;32m    682\u001b[0m                          \u001b[0mbypass_doc_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbypass_document_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m                          session=session),\n\u001b[0m\u001b[1;32m    684\u001b[0m             self.write_concern.acknowledged)\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/collection.pyc\u001b[0m in \u001b[0;36m_insert\u001b[0;34m(self, docs, ordered, check_keys, manipulate, write_concern, op_id, bypass_doc_val, session)\u001b[0m\n\u001b[1;32m    597\u001b[0m             return self._insert_one(\n\u001b[1;32m    598\u001b[0m                 \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanipulate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_concern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                 bypass_doc_val, session)\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/collection.pyc\u001b[0m in \u001b[0;36m_insert_one\u001b[0;34m(self, doc, ordered, check_keys, manipulate, write_concern, op_id, bypass_doc_val, session)\u001b[0m\n\u001b[1;32m    578\u001b[0m             result = self.__database.client._retryable_write(\n\u001b[1;32m    579\u001b[0m                 True, _insert_command, session)\n\u001b[0;32m--> 580\u001b[0;31m             \u001b[0m_check_write_command_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket_for_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/helpers.pyc\u001b[0m in \u001b[0;36m_check_write_command_response\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mwrite_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"writeErrors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0m_raise_last_write_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"writeConcernError\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pymongo/helpers.pyc\u001b[0m in \u001b[0;36m_raise_last_write_error\u001b[0;34m(write_errors)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_errors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m11000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mDuplicateKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errmsg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mWriteError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errmsg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateKeyError\u001b[0m: E11000 duplicate key error collection: tweets_db.tweets index: _id_ dup key: { : ObjectId('5b22fb2e8708dc2fe3c3124b') }"
     ]
    }
   ],
   "source": [
    "tweets_db.insert_one(tweets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import numpy as np\n",
    "\n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        global cnt\n",
    "        try:\n",
    "            tweet = json.loads(data)\n",
    "            tweets_collection.insert_one(tweet)\n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                print(cnt)\n",
    "            if cnt == max_count:\n",
    "                twitter_stream.disconnect()\n",
    "                print(\"done!\")\n",
    "                \n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "client = MongoClient()\n",
    "client.tweets_db.drop_collection('tweets')\n",
    "tweets_collection = client.tweets_db.tweets\n",
    "\n",
    "cnt = 0\n",
    "max_count = 10000\n",
    "twitter_stream = Stream(auth=auth, listener=MyListener())\n",
    "GEOBOX_US_CANADA = [-128.755117, 26.415893, -52.437305, 54.093165]\n",
    "twitter_stream.filter(locations=GEOBOX_US_CANADA, async=True)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "client.tweets_db.tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import numpy as np\n",
    "\n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        global cnt\n",
    "        try:\n",
    "            #print('data: %s' % data)\n",
    "            tweet = json.loads(data)\n",
    "            '''\n",
    "            geo = tweet['geo']\n",
    "            user_location = tweet['user']['location'] \n",
    "            city = None\n",
    "            state = None\n",
    "            \n",
    "            try:\n",
    "                second_part = user_location.split(',')[1].lower()\n",
    "                if second_part in states_abbr:\n",
    "                    state = second_part\n",
    "                elif second_part in ['us', 'usa', 'canada']:\n",
    "                    country = second_part\n",
    "                city = user_location.split(',')[0].lower()\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            if not geo and not city:\n",
    "                return\n",
    "                    \n",
    "            text = tweet['text']\n",
    "            #print(geo, \"%s, %s\" % (city, state), tweet['coordinates'], tweet['place'])\n",
    "            print(tweet['text'])\n",
    "            print\n",
    "            print(tweet['geo'])\n",
    "            print\n",
    "            print(tweet['coordinates'])\n",
    "            print\n",
    "            '''\n",
    "            coordinates = np.array(tweet['place']['bounding_box']['coordinates'][0]).mean(axis=0)\n",
    "            coordinates = coordinates.tolist()\n",
    "            #coordinates = coordinates.tolist()\n",
    "            #print(\"-----------------------------------------------------\\n\\n\")\n",
    "            \"\"\"\n",
    "            cnt = 0\n",
    "            words = [w.strip().lower() for w in text.split()]\n",
    "            for w in words:\n",
    "                if w in disaster_words:\n",
    "                    cnt += 1\n",
    "            if cnt != 0:\n",
    "                print(text)\n",
    "                print(\"%s\\n\" % cnt)\n",
    "            \"\"\"\n",
    "            \n",
    "            f_coordinates.write(\"%s,%s\\n\" % tuple(coordinates))\n",
    "            text = tweet['text'].encode('utf-8')\n",
    "            f_tweets.write(text)\n",
    "            tw = {}\n",
    "            tw['text'] = text\n",
    "            tw['loc'] = coordinates\n",
    "            tweets.append(tw)\n",
    "                \n",
    "            cnt += 1\n",
    "            if cnt % 1000 == 0:\n",
    "                print(cnt)\n",
    "            if cnt == max_count:\n",
    "                twitter_stream.disconnect()\n",
    "                print(\"done!\")\n",
    "                f_coordinates.close()\n",
    "                f_tweets.close()\n",
    "                \n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "f_coordinates = open('coordinates', 'w')\n",
    "f_tweets = open('tweets', 'w')\n",
    "tweets = []\n",
    "cnt = 0\n",
    "max_count = 10000\n",
    "twitter_stream = Stream(auth=auth, listener=MyListener())\n",
    "#twitter_stream.filter(track=['a'], async=True)\n",
    "#GEOBOX_GERMANY = [5.0770049095, 47.2982950435, 15.0403900146, 54.9039819757]\n",
    "GEOBOX_US_CANADA = [-128.755117, 26.415893, -52.437305, 54.093165]\n",
    "#twitter_stream.filter(locations=GEOBOX_GERMANY, async=True)\n",
    "#twitter_stream.filter(locations=[-6.38,49.87,1.77,55.81], async=True)\n",
    "twitter_stream.filter(locations=GEOBOX_US_CANADA, async=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_stream.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you know the 9 letter encryption on the yellow frog sticker, you get to choose the next encrypted message on the next sticker | Joplin, MO\n",
      "\n",
      "\n",
      "it was a blessing to do both https://t.co/dsHCHboYqy | Washington, DC\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "api = tweepy.API(auth)\n",
    "places = api.geo_search(query=\"USA\", granularity=\"country\", max_results=20)\n",
    "place_id = places[0].id\n",
    "\n",
    "tweets = api.search(q=\"place:%s\" % place_id)\n",
    "for tweet in tweets:\n",
    "    print tweet.text + \" | \" + (tweet.place.full_name if tweet.place else \"Undefined place\") + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmap.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gmplot\n",
    "output_html = \"my_map.html\"\n",
    "apikey = \"AIzaSyAaN6JdzBYDQuN_P8U3xjAsg4V4GwSvB6Y\"\n",
    "center_long = (GEOBOX_US_CANADA[0] + GEOBOX_US_CANADA[2])/2\n",
    "center_lat = (GEOBOX_US_CANADA[1] + GEOBOX_US_CANADA[3])/2\n",
    "gmap = gmplot.GoogleMapPlotter(center_lat, center_long, 4, apikey=apikey)\n",
    "# Polygon\n",
    "golden_gate_park_lats, golden_gate_park_lons = zip(*[\n",
    "    (37.771269, -122.511015),\n",
    "    (37.773495, -122.464830),\n",
    "    (37.774797, -122.454538),\n",
    "    (37.771988, -122.454018),\n",
    "    (37.773646, -122.440979),\n",
    "    (37.772742, -122.440797),\n",
    "    (37.771096, -122.453889),\n",
    "    (37.768669, -122.453518),\n",
    "    (37.766227, -122.460213),\n",
    "    (37.764028, -122.510347),\n",
    "    (37.771269, -122.511015)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "#gmap.plot(golden_gate_park_lats, golden_gate_park_lons, 'cornflowerblue', edge_width=10)\n",
    "#gmap.heatmap(golden_gate_park_lats, golden_gate_park_lons, radius=30)\n",
    "\n",
    "\n",
    "locations_longs, locations_lats = zip(*[[-97.662618, 27.578509], \n",
    "                                        [-97.662618, 27.895793], \n",
    "                                        [-97.202232, 27.895793], \n",
    "                                        [-97.202232, 27.578509]])\n",
    "\n",
    "#locations_longs, locations_lats = zip(*pd.read_csv('coordinates', header=None).values.tolist())\n",
    "\n",
    "coordinates_list = []\n",
    "for tw in tweets_collection.find():\n",
    "    try:\n",
    "        coordinates = np.array(tw['place']['bounding_box']['coordinates'][0]).mean(axis=0)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    coordinates_list.append(coordinates.tolist())\n",
    "\n",
    "locations_longs, locations_lats = zip(*coordinates_list)\n",
    "\n",
    "\n",
    "#gmap.plot(locations_lats, locations_longs, 'cornflowerblue', edge_width=10)\n",
    "gmap.heatmap(locations_lats, locations_longs, radius=30)\n",
    "\n",
    "\"\"\"\n",
    "# Scatter points\n",
    "top_attraction_lats, top_attraction_lons = zip(*[\n",
    "    (37.769901, -122.498331),\n",
    "    (37.768645, -122.475328),\n",
    "    (37.771478, -122.468677),\n",
    "    (37.769867, -122.466102),\n",
    "    (37.767187, -122.467496),\n",
    "    (37.770104, -122.470436)\n",
    "    ])\n",
    "gmap.scatter(top_attraction_lats, top_attraction_lons, '#3B0B39', size=40, marker=False)\n",
    "\n",
    "# Marker\n",
    "hidden_gem_lat, hidden_gem_lon = 37.770776, -122.461689\n",
    "gmap.marker(hidden_gem_lat, hidden_gem_lon, 'cornflowerblue')\n",
    "\"\"\"\n",
    "\n",
    "# Draw\n",
    "gmap.draw(output_html)\n",
    "\n",
    "import os\n",
    "os.system(\"cp %s /var/www/html\" % output_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gmplot\n",
    "import os\n",
    "\n",
    "def plot_map(coordinates_list):\n",
    "    output_html = \"my_map.html\"\n",
    "    apikey = \"AIzaSyAaN6JdzBYDQuN_P8U3xjAsg4V4GwSvB6Y\"\n",
    "    center_long = (GEOBOX_US_CANADA[0] + GEOBOX_US_CANADA[2])/2\n",
    "    center_lat = (GEOBOX_US_CANADA[1] + GEOBOX_US_CANADA[3])/2\n",
    "    gmap = gmplot.GoogleMapPlotter(center_lat, center_long, 4, apikey=apikey)\n",
    "\n",
    "    locations_longs, locations_lats = zip(*coordinates_list)\n",
    "    gmap.scatter(locations_lats, locations_longs, '#3B0B39', size=80, marker=False)\n",
    "    gmap.draw(output_html)\n",
    "\n",
    "    os.system(\"cp %s /var/www/html\" % output_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates_list = tweets_df.iloc[clusters[0]]['coordinates']\n",
    "gmap = gmplot.GoogleMapPlotter(center_lat, center_long, 4, apikey=apikey)\n",
    "locations_longs, locations_lats = zip(*coordinates_list)\n",
    "gmap.scatter(locations_lats, locations_longs, '#3B0B39', size=80, marker=False)\n",
    "gmap.draw(output_html)\n",
    "\n",
    "os.system(\"cp %s /var/www/html\" % output_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coordinates_list = tweets_df.iloc[clusters[4]]['coordinates']\n",
    "coordinates_arr = np.array(coordinates_list.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244.5207289897558"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((coordinates_arr - coordinates_arr.mean(axis=0))**2, axis=1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277.2363503730319\n",
      "284.26972289828063\n",
      "277.60512634216934\n",
      "290.9394217711046\n",
      "244.5207289897558\n",
      "271.1420478904403\n",
      "265.9798483698981\n",
      "332.13631663905517\n"
     ]
    }
   ],
   "source": [
    "for center in range(km.n_clusters):\n",
    "    coordinates_list = tweets_df.iloc[clusters[center]]['coordinates']\n",
    "    coordinates_arr = np.array(coordinates_list.tolist())\n",
    "    pos_std = np.sum((coordinates_arr - coordinates_arr.mean(axis=0))**2, axis=1).std()\n",
    "    print(pos_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head>\n",
       "<meta name=\"viewport\" content=\"initial-scale=1.0, user-scalable=no\" />\n",
       "<meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\"/>\n",
       "<title>Google Maps - pygmaps </title>\n",
       "<script type=\"text/javascript\" src=\"https://maps.googleapis.com/maps/api/js?libraries=visualization&sensor=true_or_false&key=AIzaSyAaN6JdzBYDQuN_P8U3xjAsg4V4GwSvB6Y\"></script>\n",
       "<script type=\"text/javascript\">\n",
       "\tfunction initialize() {\n",
       "\t\tvar centerlatlng = new google.maps.LatLng(37.766956, -122.438481);\n",
       "\t\tvar myOptions = {\n",
       "\t\t\tzoom: 13,\n",
       "\t\t\tcenter: centerlatlng,\n",
       "\t\t\tmapTypeId: google.maps.MapTypeId.ROADMAP\n",
       "\t\t};\n",
       "\t\tvar map = new google.maps.Map(document.getElementById(\"map_canvas\"), myOptions);\n",
       "\n",
       "var heatmap_points = [\n",
       "new google.maps.LatLng(7.993561, 53.501976),\n",
       "new google.maps.LatLng(7.993561, 53.637865),\n",
       "new google.maps.LatLng(8.171061, 53.637865),\n",
       "new google.maps.LatLng(8.171061, 53.501976),\n",
       "];\n",
       "\n",
       "var pointArray = new google.maps.MVCArray(heatmap_points);\n",
       "var heatmap;\n",
       "heatmap = new google.maps.visualization.HeatmapLayer({\n",
       "\n",
       "data: pointArray\n",
       "});\n",
       "heatmap.setMap(map);\n",
       "heatmap.set('threshold', 10);\n",
       "heatmap.set('radius', 30);\n",
       "heatmap.set('opacity', 0.600000);\n",
       "heatmap.set('dissipating', true);\n",
       "\t}\n",
       "</script>\n",
       "</head>\n",
       "<body style=\"margin:0px; padding:0px;\" onload=\"initialize()\">\n",
       "\t<div id=\"map_canvas\" style=\"width: 100%; height: 100%;\"></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "f = open(output_html)\n",
    "s = f.read()\n",
    "display(HTML('<html>\\n<head>\\n<meta name=\"viewport\" content=\"initial-scale=1.0, user-scalable=no\" />\\n<meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\"/>\\n<title>Google Maps - pygmaps </title>\\n<script type=\"text/javascript\" src=\"https://maps.googleapis.com/maps/api/js?libraries=visualization&sensor=true_or_false&key=AIzaSyAaN6JdzBYDQuN_P8U3xjAsg4V4GwSvB6Y\"></script>\\n<script type=\"text/javascript\">\\n\\tfunction initialize() {\\n\\t\\tvar centerlatlng = new google.maps.LatLng(37.766956, -122.438481);\\n\\t\\tvar myOptions = {\\n\\t\\t\\tzoom: 13,\\n\\t\\t\\tcenter: centerlatlng,\\n\\t\\t\\tmapTypeId: google.maps.MapTypeId.ROADMAP\\n\\t\\t};\\n\\t\\tvar map = new google.maps.Map(document.getElementById(\"map_canvas\"), myOptions);\\n\\nvar heatmap_points = [\\nnew google.maps.LatLng(7.993561, 53.501976),\\nnew google.maps.LatLng(7.993561, 53.637865),\\nnew google.maps.LatLng(8.171061, 53.637865),\\nnew google.maps.LatLng(8.171061, 53.501976),\\n];\\n\\nvar pointArray = new google.maps.MVCArray(heatmap_points);\\nvar heatmap;\\nheatmap = new google.maps.visualization.HeatmapLayer({\\n\\ndata: pointArray\\n});\\nheatmap.setMap(map);\\nheatmap.set(\\'threshold\\', 10);\\nheatmap.set(\\'radius\\', 30);\\nheatmap.set(\\'opacity\\', 0.600000);\\nheatmap.set(\\'dissipating\\', true);\\n\\t}\\n</script>\\n</head>\\n<body style=\"margin:0px; padding:0px;\" onload=\"initialize()\">\\n\\t<div id=\"map_canvas\" style=\"width: 100%; height: 100%;\"></div>\\n</body>\\n</html>\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Mentors Muses and Celebrities \\nDRIVE\\nLouie, Louis\\n\\nContemporary Art Museum St. Louis  \\nArt Up Late https://t.co/PvHFarRzUT'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_collection.find().next()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidv_word = TfidfVectorizer(ngram_range=(1, 3), analyzer='word', stop_words='english', min_df=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidv_vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000,\n",
    "                             min_df=2, stop_words='english',\n",
    "                             use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_id = []\n",
    "tweets_text = []\n",
    "tweets_text_processed = []\n",
    "tweets_coordinates = []\n",
    "\n",
    "for tw in tweets_collection.find():\n",
    "    try:\n",
    "        tid = tw['_id']\n",
    "        text = tw['text']\n",
    "        text_processed = preprocess(text)\n",
    "        c = np.array(tw['place']['bounding_box']['coordinates'][0]).mean(axis=0)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    tweets_id.append(tid)\n",
    "    tweets_text.append(text)\n",
    "    tweets_text_processed.append(text_processed)\n",
    "    tweets_coordinates.append(c.tolist())\n",
    "\n",
    "tweets_df = pd.DataFrame({'id': tweets_id, \n",
    "                          'text': tweets_text,\n",
    "                          'text_processed': tweets_text_processed,\n",
    "                          'coordinates': tweets_coordinates\n",
    "                         })\n",
    "tweets_df = tweets_df[['id', 'text', 'text_processed', 'coordinates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Mentors Muses and Celebrities \\nDRIVE\\nLouie, Louis\\n\\nContemporary Art Museum St. Louis  \\nArt Up Late https://t.co/PvHFarRzUT'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'mentor muse celebrities drive louie louis contemporary museum louis late https pvhfarrzut'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(tweets_df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_text_vec = tfidv_vectorizer.fit_transform(tweets_df.text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=8, init='k-means++', max_iter=100, n_init=1,\n",
    "            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 8647.707\n",
      "Iteration  1, inertia 8531.034\n",
      "Iteration  2, inertia 8518.813\n",
      "Iteration  3, inertia 8515.936\n",
      "Iteration  4, inertia 8513.775\n",
      "Iteration  5, inertia 8513.172\n",
      "Iteration  6, inertia 8512.723\n",
      "Iteration  7, inertia 8512.654\n",
      "Iteration  8, inertia 8512.620\n",
      "Converged at iteration 8: center shift 0.000000e+00 within tolerance 2.240643e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=8, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=True)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(tweets_text_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.34860753e-04, 2.64472095e-04, 2.08837541e-04, ...,\n",
       "        9.14769656e-05, 3.92042665e-04, 6.60155447e-05],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 1.28759393e-03, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "tweet_index = np.arange(len(tweets_text))\n",
    "for center in range(km.n_clusters):\n",
    "    clusters[center] = tweet_index[km.labels_ == center]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coordinates_list = tweets_df.iloc[clusters[6]]['coordinates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_map(coordinates_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-51b0b78ee48e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 8"
     ]
    }
   ],
   "source": [
    "tweets_df.iloc[clusters[8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text, stem=True):\n",
    "    result = lemmatizer.lemmatize(text, pos='v')\n",
    "    if stem:\n",
    "        result = stemmer.stem(result)\n",
    "    return result\n",
    "    \n",
    "def preprocess(text):\n",
    "    \n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS \\\n",
    "            and len(token) > 3 and len(token) < 40 \\\n",
    "            and token.isalpha:\n",
    "            result.append(lemmatize_stemming(token, stem=False))\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "#documents = [preprocess(tw['text']) for tw in tweets_collection.find()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "https\n",
      "Topic 1:\n",
      "https\n",
      "Topic 2:\n",
      "https\n",
      "Topic 3:\n",
      "https\n",
      "Topic 4:\n",
      "https\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# Data cleaning\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 5\n",
    "\n",
    "# Run NMF\n",
    "#nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=15, learning_method='online', learning_decay=.9, learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "#display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
