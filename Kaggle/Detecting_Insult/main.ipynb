{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insults  detection in Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, we consider the problem of detecting insults in social media comments. This is an important problem from a practical point of views since there are many social media sites such as youtube, yelp, etc, that are interested in detecting and filtering out comments that involve insults especially to other users. We use the data provided in [this](https://www.kaggle.com/c/detecting-insults-in-social-commentary) competition hosted by Kaggle. We follow the same setup and use the same performance criterion as in the competition to be able to compare our results to the winners of the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data and Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this competition, first \"train.csv\" was made available and contestants were ask to submit their preliminary models. Here is a random sample of the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>1</td>\n",
       "      <td>20120619044833Z</td>\n",
       "      <td>\"Hate that fucking asshole moron\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>0</td>\n",
       "      <td>20120610024150Z</td>\n",
       "      <td>\"You're wonderful just the way you are!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>0</td>\n",
       "      <td>20120618225501Z</td>\n",
       "      <td>\"...............HORSE TRAINER my AZZZZ.....TRUCE....?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>0</td>\n",
       "      <td>20120619203448Z</td>\n",
       "      <td>\"She can spew 'True Lies' because DemonCrats are Fuckin' Retarded, and she knows it. They'll take what she or President Kardashian says at face value.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0</td>\n",
       "      <td>20120527213608Z</td>\n",
       "      <td>\"What horrible trades are you talking about?\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Insult             Date  \\\n",
       "1648       1  20120619044833Z   \n",
       "552        0  20120610024150Z   \n",
       "1409       0  20120618225501Z   \n",
       "2242       0  20120619203448Z   \n",
       "102        0  20120527213608Z   \n",
       "\n",
       "                                                                                                                                                      Comment  \n",
       "1648                                                                                                                        \"Hate that fucking asshole moron\"  \n",
       "552                                                                                                                  \"You're wonderful just the way you are!\"  \n",
       "1409                                                                                                   \"...............HORSE TRAINER my AZZZZ.....TRUCE....?\"  \n",
       "2242  \"She can spew 'True Lies' because DemonCrats are Fuckin' Retarded, and she knows it. They'll take what she or President Kardashian says at face value.\"  \n",
       "102                                                                                                             \"What horrible trades are you talking about?\"  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The public and private leaderboad evaluation at this first stage was done on a test data set that later was made available as \"test_with_solutions.csv\". The contestants then submitted their final models which were evaluated on a verification set. This set was provided after the competition was disclosed as \"impermium_verification_labels.csv\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_with_solutions = pd.read_csv(\"data/test_with_solutions.csv\")\n",
    "verfication_set = pd.read_csv(\"data/impermium_verification_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in building the final model, we need to combine the \"train\" and \"test_with_solutions\" data sets and use that as training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_total = pd.concat([train, test_with_solutions.iloc[:, :-1]]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building our model we will evaluate the model performance on the \"verification\" data set above which serves as test data.\n",
    "\n",
    "Here is a glimpse of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>20120502173058Z</td>\n",
       "      <td>\"You're a moron, truth is beyond your reach\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>1182</td>\n",
       "      <td>0</td>\n",
       "      <td>20120610051138Z</td>\n",
       "      <td>\"wtfff is this shit??? he shot the fuck out for this shit.. Diz won and i only seen this clip of the battle. smh...\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>3267</td>\n",
       "      <td>1</td>\n",
       "      <td>20120619025858Z</td>\n",
       "      <td>\"You realize everytime you take a crap, you are polluting.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>1956</td>\n",
       "      <td>0</td>\n",
       "      <td>20120529032809Z</td>\n",
       "      <td>\"it wasn't long ago that our friend John Axford was dealing cellular phones... dude needs his own movie.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4923</th>\n",
       "      <td>976</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"@Kaothic kaothic haciendo hamijos? xD\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6344</th>\n",
       "      <td>2397</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"@redz_devil ugly men? so what type of guys do you usually go for then? this is football not a dating site wahh! sniff...wahh!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5293</th>\n",
       "      <td>1346</td>\n",
       "      <td>1</td>\n",
       "      <td>20120620151614Z</td>\n",
       "      <td>\"Pretense?\\n\\nOnly you leftarded, batshit crazy troglodytes would have a comment about race when in REALITY it's all about his socialist/marxist/facist bullshit way of governing.\\n\\nYou idiots can't come up with anything better of a real argument other than playing the race card.\\xa0 YOU ARE PATHETIC!!\\n\\nBesides that, you don't know me at all.\\xa0 How do you know if I'm white or black, or if I'm latino or asian.\\xa0 YOU DON'T!\\n\\nTry again you mouthy little punk!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5045</th>\n",
       "      <td>1098</td>\n",
       "      <td>0</td>\n",
       "      <td>20120528221414Z</td>\n",
       "      <td>\"Joyce does not have all star numbers, come on homer.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2981</th>\n",
       "      <td>2981</td>\n",
       "      <td>0</td>\n",
       "      <td>20120611234239Z</td>\n",
       "      <td>\"a lot like you and your life huh?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4694</th>\n",
       "      <td>747</td>\n",
       "      <td>1</td>\n",
       "      <td>20120619131251Z</td>\n",
       "      <td>\"Vacman, Foxtitues is lame, corny.\\xa0 I know you are proud of yourself for thinking you invented the term but I have seen it in other folk's posts.\\xa0 It just makes me groan.\\xa0 I also don't like libtard, Rachel Madcow, etc.\\xa0 These terms are tiresome.\\xa0 And you need to\\xa0quit watching CNN, MSNBC, Anderson Cooper, Ed Schultz, etc.\\xa0 You really need to develop some views of your own.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Insult             Date  \\\n",
       "55       55       1  20120502173058Z   \n",
       "1182   1182       0  20120610051138Z   \n",
       "3267   3267       1  20120619025858Z   \n",
       "5903   1956       0  20120529032809Z   \n",
       "4923    976       0              NaN   \n",
       "6344   2397       0              NaN   \n",
       "5293   1346       1  20120620151614Z   \n",
       "5045   1098       0  20120528221414Z   \n",
       "2981   2981       0  20120611234239Z   \n",
       "4694    747       1  20120619131251Z   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Comment  \n",
       "55                                                                                                                                                                                                                                                                                                                                                                                                                                             \"You're a moron, truth is beyond your reach\"  \n",
       "1182                                                                                                                                                                                                                                                                                                                                                                   \"wtfff is this shit??? he shot the fuck out for this shit.. Diz won and i only seen this clip of the battle. smh...\"  \n",
       "3267                                                                                                                                                                                                                                                                                                                                                                                                                            \"You realize everytime you take a crap, you are polluting.\"  \n",
       "5903                                                                                                                                                                                                                                                                                                                                                                              \"it wasn't long ago that our friend John Axford was dealing cellular phones... dude needs his own movie.\"  \n",
       "4923                                                                                                                                                                                                                                                                                                                                                                                                                                                \"@Kaothic kaothic haciendo hamijos? xD\"  \n",
       "6344                                                                                                                                                                                                                                                                                                                                                        \"@redz_devil ugly men? so what type of guys do you usually go for then? this is football not a dating site wahh! sniff...wahh!\"  \n",
       "5293  \"Pretense?\\n\\nOnly you leftarded, batshit crazy troglodytes would have a comment about race when in REALITY it's all about his socialist/marxist/facist bullshit way of governing.\\n\\nYou idiots can't come up with anything better of a real argument other than playing the race card.\\xa0 YOU ARE PATHETIC!!\\n\\nBesides that, you don't know me at all.\\xa0 How do you know if I'm white or black, or if I'm latino or asian.\\xa0 YOU DON'T!\\n\\nTry again you mouthy little punk!\"  \n",
       "5045                                                                                                                                                                                                                                                                                                                                                                                                                                 \"Joyce does not have all star numbers, come on homer.\"  \n",
       "2981                                                                                                                                                                                                                                                                                                                                                                                                                                                    \"a lot like you and your life huh?\"  \n",
       "4694                                                                           \"Vacman, Foxtitues is lame, corny.\\xa0 I know you are proud of yourself for thinking you invented the term but I have seen it in other folk's posts.\\xa0 It just makes me groan.\\xa0 I also don't like libtard, Rachel Madcow, etc.\\xa0 These terms are tiresome.\\xa0 And you need to\\xa0quit watching CNN, MSNBC, Anderson Cooper, Ed Schultz, etc.\\xa0 You really need to develop some views of your own.\"  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 1000)\n",
    "train_total.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>1959</td>\n",
       "      <td>1</td>\n",
       "      <td>20120529105937Z</td>\n",
       "      <td>\"You're analysis is exceptionally American. And by that I mean it's entirely grounded in fact-challenged star-spangled retardation- entirely voluntary in nature.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>20120530022806Z</td>\n",
       "      <td>\"Again:\\xa0 I wouldn't count on it.\\n\\nAnyone who continues to support the current empty suit is either totally deluded or a_traitor.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5844</th>\n",
       "      <td>1897</td>\n",
       "      <td>1</td>\n",
       "      <td>20120528034620Z</td>\n",
       "      <td>\"FANS ARE SOOOO BLIND DUMB. refs handed game to spurs??? THUNDER MADE two more free throws than the spurs (19 to 17) AND SPURS STILL WON! shut it haters\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6376</th>\n",
       "      <td>2429</td>\n",
       "      <td>0</td>\n",
       "      <td>20120528043920Z</td>\n",
       "      <td>\"I don't know what to think about LW1 - it's possible that she's just not a facebook type person.  Maybe she should change her settings so she doesn't get the updates and photos.  That way she can look at their page if she feels like it but she won't be seeing more than she really wants to know. \\n\\nI have to agree though that writing about awesome presents you received or how much you really love, love, love your awesome BF, or telling everyone about your Valentines plans - seems a little like what CH called over sharing.  I guess because it's not the kind of the info that I would think about announcing.  For example, if my husband gives me an awesome gift - I don't rush to the phone to call everyone I know and tell them or even do a mass email to let everyone know.  Same with plans for special occasions.  \\n\\nIf someone asks me - then of course I'll tell them.  But the announcing to the world thing does seem a little over the top when it comes to things like that.  But then, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4214</th>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>20120618211714Z</td>\n",
       "      <td>\"Fucking idiots. They should all be in jail. Make things worse for all of us responsible and respectful cyclists.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>3686</td>\n",
       "      <td>0</td>\n",
       "      <td>20120530001221Z</td>\n",
       "      <td>\"What's he going to say? \"Nope sorry. Next question.\". Then they would of criticized him for not answering a simple question. its lose-lose for him.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"He had the potential but blew every one of his chances.\\\\n\\\\n \\\\n\\\\nNot good enough for Arsenal and he'll not be missed.\\\\n\\\\n \\\\n\\\\nI expect him to be at Spurs in 10 years or so.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>3483</td>\n",
       "      <td>1</td>\n",
       "      <td>20120609175054Z</td>\n",
       "      <td>\"You really are NOTHING but an animal, aren't you?\\n\\nHave a nice day, I sincerely hope you manage to get some help for your anger issues.\\n\\nYou are OBVIOUSLY DEEPLY DISTURBED and you have my sympathy.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>3275</td>\n",
       "      <td>0</td>\n",
       "      <td>20120609182832Z</td>\n",
       "      <td>\"I like politicians who wear their biases on their sleeves *much* better than the slick ones, who say only what the crowd before them wants to hear.\\n\\nLike *both* Presidential nominees this go round, and like Governor McDonnell of Virginia.\\n\\nMcDonnell has his vision set on higher office, and doesn't want to fuck it up right now.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>20120530141243Z</td>\n",
       "      <td>\"One thing that disappointed me about DT last night... since my cable was out I was drawn to the threads and it was disgusting. People on here cussing at brooks, the refs, calling for whole sale changes.\\n\\xa0\\nTo those where in the game thread last night. you should be ashamed. Knee-jerking and lambasting our team for every single thing that didn't go right. Seriously. I expected more from this group.\\n\\xa0\\nyou want to complain? You want to give your advice? Then do that in a constructive manner. The time for debating whole sale changes and coaching extensions and amnestying Perk and what not will come later. But right now lets focus our energy on support OUR TEAM and looking at what they can do to improve.\\n\\xa0\\nTHUNDER UP!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Insult             Date  \\\n",
       "1959   1959       1  20120529105937Z   \n",
       "2450   2450       0  20120530022806Z   \n",
       "5844   1897       1  20120528034620Z   \n",
       "6376   2429       0  20120528043920Z   \n",
       "4214    267       0  20120618211714Z   \n",
       "3686   3686       0  20120530001221Z   \n",
       "4211    264       0              NaN   \n",
       "3483   3483       1  20120609175054Z   \n",
       "3275   3275       0  20120609182832Z   \n",
       "4589    642       0  20120530141243Z   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Comment  \n",
       "1959                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"You're analysis is exceptionally American. And by that I mean it's entirely grounded in fact-challenged star-spangled retardation- entirely voluntary in nature.\"  \n",
       "2450                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"Again:\\xa0 I wouldn't count on it.\\n\\nAnyone who continues to support the current empty suit is either totally deluded or a_traitor.\"  \n",
       "5844                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \"FANS ARE SOOOO BLIND DUMB. refs handed game to spurs??? THUNDER MADE two more free throws than the spurs (19 to 17) AND SPURS STILL WON! shut it haters\"  \n",
       "6376  \"I don't know what to think about LW1 - it's possible that she's just not a facebook type person.  Maybe she should change her settings so she doesn't get the updates and photos.  That way she can look at their page if she feels like it but she won't be seeing more than she really wants to know. \\n\\nI have to agree though that writing about awesome presents you received or how much you really love, love, love your awesome BF, or telling everyone about your Valentines plans - seems a little like what CH called over sharing.  I guess because it's not the kind of the info that I would think about announcing.  For example, if my husband gives me an awesome gift - I don't rush to the phone to call everyone I know and tell them or even do a mass email to let everyone know.  Same with plans for special occasions.  \\n\\nIf someone asks me - then of course I'll tell them.  But the announcing to the world thing does seem a little over the top when it comes to things like that.  But then, that...  \n",
       "4214                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \"Fucking idiots. They should all be in jail. Make things worse for all of us responsible and respectful cyclists.\"  \n",
       "3686                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \"What's he going to say? \"Nope sorry. Next question.\". Then they would of criticized him for not answering a simple question. its lose-lose for him.\"  \n",
       "4211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \"He had the potential but blew every one of his chances.\\\\n\\\\n \\\\n\\\\nNot good enough for Arsenal and he'll not be missed.\\\\n\\\\n \\\\n\\\\nI expect him to be at Spurs in 10 years or so.\"  \n",
       "3483                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \"You really are NOTHING but an animal, aren't you?\\n\\nHave a nice day, I sincerely hope you manage to get some help for your anger issues.\\n\\nYou are OBVIOUSLY DEEPLY DISTURBED and you have my sympathy.\"  \n",
       "3275                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \"I like politicians who wear their biases on their sleeves *much* better than the slick ones, who say only what the crowd before them wants to hear.\\n\\nLike *both* Presidential nominees this go round, and like Governor McDonnell of Virginia.\\n\\nMcDonnell has his vision set on higher office, and doesn't want to fuck it up right now.\"  \n",
       "4589                                                                                                                                                                                                                                                                       \"One thing that disappointed me about DT last night... since my cable was out I was drawn to the threads and it was disgusting. People on here cussing at brooks, the refs, calling for whole sale changes.\\n\\xa0\\nTo those where in the game thread last night. you should be ashamed. Knee-jerking and lambasting our team for every single thing that didn't go right. Seriously. I expected more from this group.\\n\\xa0\\nyou want to complain? You want to give your advice? Then do that in a constructive manner. The time for debating whole sale changes and coaching extensions and amnestying Perk and what not will come later. But right now lets focus our energy on support OUR TEAM and looking at what they can do to improve.\\n\\xa0\\nTHUNDER UP!\"  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_total.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we are provided with the text of the comments as well the date the comments were posted. Also the Insult column shows whether the comments is an insult. This is what we are required to predict for the test data.\n",
    "\n",
    "We also see that the comments are not cleaned. For example the non-breaking spaces appear as \"\\xa0\" and should be removed. Also there characters like \"\\n\" and \"\\\\n\" which should be replaced by a white space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct a transformer which takes the comments as input and transforms them to cleaned comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-fa43af8057a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_total_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_total_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'comments' is not defined"
     ]
    }
   ],
   "source": [
    "train_total_clean = train_total.copy()\n",
    "train_total_clean.Comment = comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        comments_clean = []\n",
    "        for c in X:\n",
    "            c = c.replace('\\\\\\\\', '\\\\')\n",
    "            c = c.replace('\\\\n', ' ')\n",
    "            c = re.sub(r'http[s]?://[^\\s]*', ' ', c)\n",
    "            c = re.sub(r'\\\\x[0-9a-f]{2}', ' ', c)\n",
    "            c = re.sub(r'\\\\u[0-9a-f]{4}', ' ', c)\n",
    "            c = re.sub(r'[-\"]', '', c)\n",
    "            c = re.sub(r'[:*#%&,.?!\\']', ' ', c)\n",
    "            c = re.sub(r\"(.)\\1{2,}\", '\\g<1>', c)\n",
    "            #c = re.sub(r'@[^ ]*', ' ', c)\n",
    "            #c = re.sub(r'[0-9]+', ' ', c)\n",
    "            c = \" \".join([wordnet_lemmatizer.lemmatize(wordnet_lemmatizer.lemmatize(w, pos='v')).lower() \n",
    "                          for w in c.split()])\n",
    "\n",
    "            #c = \" \".join([w.lower() for w in c.split()])# if 3<=len(w)<=40])\n",
    "            comments_clean.append(c)\n",
    "        return comments_clean\n",
    "    \n",
    "preprocessor = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'he have the potential but blow every one of his chance not good enough for arsenal and he ll not be miss i expect him to be at spurs in 10 year or so']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit_transform([train_total.iloc[4211]['Comment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"He had the potential but blew every one of his chances.\\\\n\\\\n \\\\n\\\\nNot good enough for Arsenal and he'll not be missed.\\\\n\\\\n \\\\n\\\\nI expect him to be at Spurs in 10 years or so.\"\n"
     ]
    }
   ],
   "source": [
    "print(train_total.iloc[4211]['Comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer above performs the following cleaning operations on the data:\n",
    "* remove double-scape and carriage-return using ```c = c.replace('\\\\\\\\', '\\\\')``` and ```c = c.replace('\\\\n', ' ')``` \n",
    "* remove urls using ```c = re.sub(r'http[s]?://[^\\s]*', ' ', c)```\n",
    "* remove punctuation using ```c = re.sub(r'[:*#%&,.?!\\']', ' ', c)```\n",
    "* remove non-ascii and unicode characters using ```c = re.sub(r'\\\\x[0-9a-f]{2}', ' ', c)``` and ```c = re.sub(r'\\\\u[0-9a-f]{4}', ' ', c)```\n",
    "* replace repeated characters with a single character so that e.g. word \"heeelllll\" will be converted to \"hell\"\n",
    "* replace each word with its lemmatized version and convert all words to lower case\n",
    "             \n",
    "Here we can see the original and cleaned comments side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Comment_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6378</th>\n",
       "      <td>\"I wear Gucci, I wear Prada, at the same damn time!\"</td>\n",
       "      <td>i wear gucci i wear prada at the same damn time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6169</th>\n",
       "      <td>\"The D-Bags are absolutely atrocious from top to bottom - horrific pitching and zero offense.\"</td>\n",
       "      <td>the dbags be absolutely atrocious from top to bottom horrific pitch and zero offense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>\"All of y'all sound like some lil bitches! Yeezy this ...Yeezy that...are they gonna...are they not gonna drop....man shut the fuck already. Every last one of y'all acting like some hypebeats. Pull your skirts down and your pants up and stop wettin yourself for a fuckin pair of shoes!\"</td>\n",
       "      <td>all of y all sound like some lil bitch yeezy this yeezy that be they gonna be they not gonna drop man shut the fuck already every last one of y all act like some hypebeats pull your skirt down and your pant up and stop wettin yourself for a fuckin pair of shoe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6325</th>\n",
       "      <td>\"LOL...Why are you wasting your time reading the comments?\"</td>\n",
       "      <td>lol why be you waste your time read the comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>\"@daredevil241\\xa0\\xa0@tomislavgrujic\\xa0yea well when you do make sure you punch him with your right hand otherwise it might not be authentic enough for him\"</td>\n",
       "      <td>@daredevil241 @tomislavgrujic yea well when you do make sure you punch him with your right hand otherwise it might not be authentic enough for him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>\"you are a mental midget you can't be\\xa0 a traitor if you aren't a citizen.\"</td>\n",
       "      <td>you be a mental midget you can t be a traitor if you aren t a citizen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>\"you're not stupid, just dumb\"</td>\n",
       "      <td>you re not stupid just dumb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5952</th>\n",
       "      <td>\"GET YER TOWELS READY, ROENICK'S WARMING UP THE TEAR DUCTS\"</td>\n",
       "      <td>get yer towels ready roenick s warming up the tear ducts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5224</th>\n",
       "      <td>\"He should be deported immediately, along with any others who would do us harm. And if the ECHR don't like it, tough. And when Britain really was Great, he would have been. But I am fully aware that our government haven't got what it takes to put the safety of the British people above kowtowing to Europe.\"</td>\n",
       "      <td>he should be deport immediately along with any others who would do u harm and if the echr don t like it tough and when britain really be great he would have be but i be fully aware that our government haven t get what it take to put the safety of the british people above kowtow to europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>\"suck a dick pussy ass nigga bitch!!!\"</td>\n",
       "      <td>suck a dick pussy as nigga bitch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                  Comment                                                                                                                                                                                                                                                                                     Comment_clean\n",
       "6378                                                                                                                                                                                                                                                                 \"I wear Gucci, I wear Prada, at the same damn time!\"                                                                                                                                                                                                                                                   i wear gucci i wear prada at the same damn time\n",
       "6169                                                                                                                                                                                                                       \"The D-Bags are absolutely atrocious from top to bottom - horrific pitching and zero offense.\"                                                                                                                                                                                                              the dbags be absolutely atrocious from top to bottom horrific pitch and zero offense\n",
       "6104                       \"All of y'all sound like some lil bitches! Yeezy this ...Yeezy that...are they gonna...are they not gonna drop....man shut the fuck already. Every last one of y'all acting like some hypebeats. Pull your skirts down and your pants up and stop wettin yourself for a fuckin pair of shoes!\"                              all of y all sound like some lil bitch yeezy this yeezy that be they gonna be they not gonna drop man shut the fuck already every last one of y all act like some hypebeats pull your skirt down and your pant up and stop wettin yourself for a fuckin pair of shoe\n",
       "6325                                                                                                                                                                                                                                                          \"LOL...Why are you wasting your time reading the comments?\"                                                                                                                                                                                                                                                   lol why be you waste your time read the comment\n",
       "432                                                                                                                                                        \"@daredevil241\\xa0\\xa0@tomislavgrujic\\xa0yea well when you do make sure you punch him with your right hand otherwise it might not be authentic enough for him\"                                                                                                                                                @daredevil241 @tomislavgrujic yea well when you do make sure you punch him with your right hand otherwise it might not be authentic enough for him\n",
       "4119                                                                                                                                                                                                                                        \"you are a mental midget you can't be\\xa0 a traitor if you aren't a citizen.\"                                                                                                                                                                                                                             you be a mental midget you can t be a traitor if you aren t a citizen\n",
       "2822                                                                                                                                                                                                                                                                                       \"you're not stupid, just dumb\"                                                                                                                                                                                                                                                                       you re not stupid just dumb\n",
       "5952                                                                                                                                                                                                                                                          \"GET YER TOWELS READY, ROENICK'S WARMING UP THE TEAR DUCTS\"                                                                                                                                                                                                                                          get yer towels ready roenick s warming up the tear ducts\n",
       "5224  \"He should be deported immediately, along with any others who would do us harm. And if the ECHR don't like it, tough. And when Britain really was Great, he would have been. But I am fully aware that our government haven't got what it takes to put the safety of the British people above kowtowing to Europe.\"  he should be deport immediately along with any others who would do u harm and if the echr don t like it tough and when britain really be great he would have be but i be fully aware that our government haven t get what it take to put the safety of the british people above kowtow to europe\n",
       "352                                                                                                                                                                                                                                                                                \"suck a dick pussy ass nigga bitch!!!\"                                                                                                                                                                                                                                                                  suck a dick pussy as nigga bitch"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample = train_total.sample(10)\n",
    "train_sample['Comment_clean'] = preprocessor.fit_transform(train_sample['Comment'])\n",
    "train_sample[['Comment', 'Comment_clean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to extract feature from text is text vectorization. In this approach, first a bag-of-word representation of the documents is build which contains all the words that appear in any of the documents. Then the number of times each word appears in a document is calculated and is used as a feature vector to represent the document. Optionally, we can divide the word counts by the frequency of each word appearing in all the documents combine. This allows us to reduce the effect of very common words which do not convey much information. A detailed explanation of text vectorization techniques can be found [here](http://scikit-learn.org/stable/modules/feature_extraction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv_char = TfidfVectorizer(ngram_range=(1, 5), analyzer='char', stop_words='english')\n",
    "tfidv_word = TfidfVectorizer(ngram_range=(1, 3), analyzer='word', stop_words='english', min_df=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its plain form, text vectorization only considers word frequency and does not take into account the position of words relative to each other. In other words, it does not take into account the text \"context\". To alleviate this problem, as seen above, we can use the ngram_range parameter of the vectorizer. We also note that to extract richer set of features, we have used two vectorizers, one that works of words and one that works on characters.\n",
    "\n",
    "In addition to standard features, we develop the following feature extractor transformer which extract features that are directly related to the problem of insult detection. This transformer extracts the following features:\n",
    "\n",
    "* The count of bad words used the in comment.\n",
    "* The number of positional tags such as verb, noun, adjective, etc, obtained using ntlk.\n",
    "* The number of at mentions in the comment since we are asked to detect insults directed to other members as apposed to public figures like politicians or celebrities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "class FeatureExtractor(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        with open('bad_words.txt') as f:\n",
    "            self.badwords = set(w.strip() for w in f.readlines())\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.fit_transform(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.fit_transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "\n",
    "        pos_tags = [\"VB\", \"JJ\", \"RB\", \"NN\"]\n",
    "        def pos_tag_count(comment):\n",
    "            pos = nltk.pos_tag(word_tokenize(comment))\n",
    "            d = {}\n",
    "            for tag in pos_tags:\n",
    "                d[tag] = 0\n",
    "            for p in pos:\n",
    "                for tag in pos_tags:\n",
    "                    if p[1].startswith(tag):\n",
    "                        d[tag] += 1\n",
    "            return d.values()\n",
    "    \n",
    "        def bad_word_count(comment):\n",
    "            cnt = 0\n",
    "            for w in comment.split():\n",
    "                if w.strip() in self.badwords:\n",
    "                    cnt += 1\n",
    "            return cnt\n",
    "        \n",
    "        def at_mention_count(comment):\n",
    "            return len(re.findall(r'@[^\\s]+', comment))\n",
    "        \n",
    "        comments_features = []\n",
    "        for c in X:\n",
    "            features = pos_tag_count(c)\n",
    "            features.append(bad_word_count(c))\n",
    "            features.append(at_mention_count(c))\n",
    "            comments_features.append(features)\n",
    "        \n",
    "        return sparse.csr_matrix(comments_features)\n",
    "                    \n",
    "prep = Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to combine the extracted features using FeatureUnion and create a pipeline which consists of preprocessing and cleaning, feature extraction, and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "tfidv_char = TfidfVectorizer(analyzer='char', stop_words='english')\n",
    "tfidv_word = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "#tfidv_word = None\n",
    "fe = FeatureExtractor()\n",
    "\n",
    "fu = FeatureUnion([('fe', fe), ('tfidv_char', tfidv_char), ('tfidv_word', tfidv_word)])\n",
    "#clf_rf = RandomForestClassifier()\n",
    "clf_rf = LogisticRegression(C=20)\n",
    "\n",
    "estimators = [('prep', prep), ('fu', fu), ('clf', clf_rf)]\n",
    "pl = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric used for the competition is ROC AUC score which can be calculated as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def scoring(estimator, X, y):\n",
    "    y_pred = estimator.predict_proba(X)\n",
    "    return roc_auc_score(y, y_pred[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scoring metric we can calculated the performance our pipeline using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88045897, 0.87067166, 0.88132557])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(pl, train_total.Comment, train_total.Insult, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation score is looks great and have small variations across the cross-validation folds. But we should keep in mind that this is out local validation score and does not necessarily translate directly to leader-board score. Nevertheless, we can use the local score as a proxy for the leader board score and employ it to improve the classification performance. \n",
    "\n",
    "In the following we use grid-search to perform an exhaustive search over a range of parameters for feature extraction and classification steps in out data pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-59a6b055d1f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m              }\n\u001b[1;32m     11\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInsult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.wait(): got it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {'clf__C':np.logspace(-1, 2, 5),\n",
    "              'fu__tfidv_word__ngram_range': [(1, 1), (1, 2)],\n",
    "              'fu__tfidv_char__ngram_range': [(1, 1), (1, 3)],\n",
    "              'fu__tfidv_char__norm': ['l1', 'l2'],\n",
    "              'fu__tfidv_char__min_df': [1, 2, 4],\n",
    "              'fu__tfidv_char__stop_words': ['english', None],\n",
    "             }\n",
    "gs = GridSearchCV(pl, param_grid, scoring=scoring, n_jobs=4)\n",
    "gs.fit(train_total.Comment, train_total.Insult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 17.78279410038923,\n",
       " 'fu__tfidv_char__min_df': 1,\n",
       " 'fu__tfidv_char__ngram_range': (1, 7),\n",
       " 'fu__tfidv_char__norm': 'l2',\n",
       " 'fu__tfidv_char__stop_words': 'english'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9096002277058917"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90370966, 0.90502588, 0.91382083])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, train_total.Comment, train_total.Insult, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8213229 , 0.81389727, 0.80186759])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, verfication_set.Comment, verfication_set.Insult, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8135985907248915"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_ver_pred = clf.predict_proba(verfication_set.Comment)[:, 1]\n",
    "y_ver = verfication_set.Insult\n",
    "\n",
    "roc_auc_score(y_ver, y_ver_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9706875912593864"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8177848818842078/0.84248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {'clf__C':np.logspace(-1, 2, 5)}\n",
    "gs = GridSearchCV(pl, param_grid, scoring=scoring, n_jobs=4)\n",
    "gs.fit(train_total.Comment, train_total.Insult)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
