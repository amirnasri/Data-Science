{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import check_random_state\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 20\n",
    "x = np.arange(n)\n",
    "rs = check_random_state(0)\n",
    "y = rs.randint(-10, 10, size=(n,)) + 50. * np.log(1 + np.arange(n))\n",
    "x = x.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures()\n",
    "lr = LinearRegression()\n",
    "pl = Pipeline([('pf', pf), ('lr', lr)])\n",
    "x, y = shuffle(x, y)\n",
    "x_tr, x_test, y_tr, y_test = train_test_split(x, y, test_size=0.33)\n",
    "n_folds = 5\n",
    "degrees = range(0, 10)\n",
    "kf = KFold(x_tr.shape[0], n_folds)\n",
    "tr_scores = np.zeros((n_folds, len(degrees)))\n",
    "cv_scores = np.zeros((n_folds, len(degrees)))\n",
    "for i, (tr_ind, cv_ind) in enumerate(kf):\n",
    "    for j, d in enumerate(degrees):\n",
    "        pl.set_params(pf__degree = d).fit(x_tr[tr_ind], y_tr[tr_ind])\n",
    "        tr_scores[i, j] = pl.score(x_tr[tr_ind], y_tr[tr_ind])\n",
    "        cv_scores[i, j] = pl.score(x_tr[cv_ind], y_tr[cv_ind])\n",
    "tr_scores = tr_scores.mean(axis=0)\n",
    "cv_scores = cv_scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.54396159e+00,   3.95091101e-01,   7.50568551e-01,\n",
       "         6.76287009e-01,   6.89717985e-01,   7.35884690e-01,\n",
       "         1.58794618e-01,   8.37294314e-01,  -1.24330794e+02,\n",
       "        -1.04037600e+03])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_ = np.argmax(cv_scores)\n",
    "pl.set_params(pf__degree = d_)\n",
    "pl.fit(x_tr, y_tr)\n",
    "score_ = pl.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(degrees, cv_scores, 'r-')\n",
    "plt.plot(degrees, tr_scores, 'b-')\n",
    "plt.scatter(d_, score_)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'r.')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "# generate some data to play with\n",
    "X, y = samples_generator.make_classification(\n",
    "    n_informative=5, n_redundant=0, random_state=42)\n",
    "# ANOVA SVM-C\n",
    "anova_filter = SelectKBest(f_regression, k=5)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline is a cascade of transforms with a final estimator.\n",
    "\n",
    "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\n",
    "\n",
    "Pipeline([('tf1', tf1), ('tf2', tf2), ('est', est)]).fit(X, y) does the following:\n",
    "tf1.fit(X, y)\n",
    "X1 = tf1.transform(X)\n",
    "(if tf1 implements fit_transform then X1 = tf1.fit_transform(X, y))\n",
    "\n",
    "{ t.fit_transform(X, y, \\*\\*fit_params) = t.fit(X, y, \\*\\*fit_params).transform(X) }\n",
    "\n",
    "tf2.fit(X1, y)\n",
    "X2 = tf2.transform(X)\n",
    "\n",
    "est.fit(X2, y)\n",
    "\n",
    "\n",
    "Applying fit with y to tfx such as tfx.fit(X, y) may not make sense because y is the final outcome, but at least in some cases tfx.fit(X, y) ignores y and only fits to X, e.g., CounterVectorizer which learn the dictionay from only X.\n",
    "\n",
    "Prediction step normally does not involve fitting, and it applies to a fitted transfrom. In this case pipeline.predict(X) does est.predict(tf2.transform(tf1.transform(X))), i.e., it applies the transforms to X (without fitting) and the calls est.predict on the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator API summary:\n",
    "* est.fit(X, y): fit estimator to data X, y. Return the fitted estimator (internal parameters are fitted).\n",
    "* est.transform(X): transforms X to another matrix (this matrix is not necessarily similar to y, e.g., in CountVectorizer)\n",
    "* est.fit_transform(X, y) = est.fit(X, y).transform(X)\n",
    "* est.predict(X, y): transform X to something similar to y!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape, iris.target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Side note: An example implementation of train_test_split\n",
    "# Note: The orig implementation uses iterators and generators. \n",
    "#   Maybe better to use those.\n",
    "import numpy as np\n",
    "def train_test_split(*arrays, **options):\n",
    "        test_size = options.pop('test_size', None)\n",
    "        if test_size is None:\n",
    "            test_size = .1\n",
    "        if len(options) != 0:\n",
    "            raise TypeError(\"Invalid parameters passed: %s\" % str(options))\n",
    "        if len(arrays) == 0:\n",
    "            raise ValueError(\"At least one array required as input\")\n",
    "        \n",
    "        arrays_ = []\n",
    "        m = arrays[0].shape[0]\n",
    "        for a in arrays:\n",
    "            if a.shape[0] != m:\n",
    "                raise ValueError(\"All arrays must have same first dimension\")\n",
    "            if len(a.shape) == 1:\n",
    "                arrays_.append(np.atleast_2d(a).T)\n",
    "            else:\n",
    "                arrays_.append(a)\n",
    "                \n",
    "        p = np.random.permutation(m)\n",
    "        test_ind_max = int(test_size * m)\n",
    "        test_ind = p[:test_ind_max]\n",
    "        train_ind = p[test_ind_max:]\n",
    "        l = []\n",
    "        for a in arrays_:\n",
    "            # Note: a[r1:r2] makes a copy, so modifying the output of this function\n",
    "            # does not affect the original arrays\n",
    "            l.extend([a[train_ind, :], a[test_ind, :]])\n",
    "        return l\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[i for i in train_test_split(iris.data, iris.target)]\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "x1 = [i for i in train_test_split(x, test_size = .5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.\n",
    "\n",
    "The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96666667,  1.        ,  0.96666667,  0.96666667,  1.        ])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "scores                                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use other cross validation strategies by passing a cross validation **iterable** instead, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97777778,  0.97777778,  1.        ,  0.95555556,  1.        ,\n",
       "        0.97777778,  0.97777778,  1.        ,  0.97777778,  0.97777778])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "n_samples = iris.data.shape[0]\n",
    "cv = ShuffleSplit(n_samples, test_size=0.3, random_state=0)\n",
    "cross_val_score(clf, iris.data, iris.target, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97333333333333338"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import metrics\n",
    "predicted = cross_val_predict(clf, iris.data, iris.target, cv=10)\n",
    "print metrics.accuracy_score(iris.target, predicted) \n",
    "np.mean(cross_val_score(clf, iris.data, iris.target, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation Iterable:**\n",
    "* KFold\n",
    "* LeaveOneOut\n",
    "* ShuffleSplit (in later versions)\n",
    "* StratifiedKFold\n",
    "* GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyper-parameters of an estimator\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n",
    "\n",
    "\n",
    "Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use: estimator.get_params()\n",
    "\n",
    "A search consists of:\n",
    "\n",
    "* an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "* a parameter space;\n",
    "* a method for searching or sampling candidates;\n",
    "* a cross-validation scheme; and\n",
    "* a score function.\n",
    "\n",
    "Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, **GridSearchCV** exhaustively considers all parameter combinations, while **RandomizedSearchCV** can sample a given number of candidates from a parameter space with a specified distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import pandas as pd\n",
    "iris = datasets.load_iris()\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[.1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GridSearchCV_(estimator, params_grid):\n",
    "    \n",
    "    grid_search_dims = [len(v) for v in param_grid.values]\n",
    "    res = np.zeros(*grid_search_dims)\n",
    "    np.ndenumerate(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
