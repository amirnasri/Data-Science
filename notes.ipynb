{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import check_random_state\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 20\n",
    "x = np.arange(n)\n",
    "rs = check_random_state(0)\n",
    "y = rs.randint(-10, 10, size=(n,)) + 50. * np.log(1 + np.arange(n))\n",
    "x = x.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures()\n",
    "lr = LinearRegression()\n",
    "pl = Pipeline([('pf', pf), ('lr', lr)])\n",
    "x, y = shuffle(x, y)\n",
    "x_tr, x_test, y_tr, y_test = train_test_split(x, y, test_size=0.33)\n",
    "n_folds = 5\n",
    "degrees = range(0, 10)\n",
    "kf = KFold(x_tr.shape[0], n_folds)\n",
    "tr_scores = np.zeros((n_folds, len(degrees)))\n",
    "cv_scores = np.zeros((n_folds, len(degrees)))\n",
    "for i, (tr_ind, cv_ind) in enumerate(kf):\n",
    "    for j, d in enumerate(degrees):\n",
    "        pl.set_params(pf__degree = d).fit(x_tr[tr_ind], y_tr[tr_ind])\n",
    "        tr_scores[i, j] = pl.score(x_tr[tr_ind], y_tr[tr_ind])\n",
    "        cv_scores[i, j] = pl.score(x_tr[cv_ind], y_tr[cv_ind])\n",
    "tr_scores = tr_scores.mean(axis=0)\n",
    "cv_scores = cv_scores.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.54396159e+00,   3.95091101e-01,   7.50568551e-01,\n",
       "         6.76287009e-01,   6.89717985e-01,   7.35884690e-01,\n",
       "         1.58794618e-01,   8.37294314e-01,  -1.24330794e+02,\n",
       "        -1.04037600e+03])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_ = np.argmax(cv_scores)\n",
    "pl.set_params(pf__degree = d_)\n",
    "pl.fit(x_tr, y_tr)\n",
    "score_ = pl.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(degrees, cv_scores, 'r-')\n",
    "plt.plot(degrees, tr_scores, 'b-')\n",
    "plt.scatter(d_, score_)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'r.')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "# generate some data to play with\n",
    "X, y = samples_generator.make_classification(\n",
    "    n_informative=5, n_redundant=0, random_state=42)\n",
    "# ANOVA SVM-C\n",
    "anova_filter = SelectKBest(f_regression, k=5)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline is a cascade of transforms with a final estimator.\n",
    "\n",
    "Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\n",
    "\n",
    "Pipeline([('tf1', tf1), ('tf2', tf2), ('est', est)]).fit(X, y) does the following:\n",
    "tf1.fit(X, y)\n",
    "X1 = tf1.transform(X)\n",
    "(if tf1 implements fit_transform then X1 = tf1.fit_transform(X, y))\n",
    "\n",
    "{ t.fit_transform(X, y, \\*\\*fit_params) = t.fit(X, y, \\*\\*fit_params).transform(X) }\n",
    "\n",
    "tf2.fit(X1, y)\n",
    "X2 = tf2.transform(X)\n",
    "\n",
    "est.fit(X2, y)\n",
    "\n",
    "\n",
    "Applying fit with y to tfx such as tfx.fit(X, y) may not make sense because y is the final outcome, but at least in some cases tfx.fit(X, y) ignores y and only fits to X, e.g., CounterVectorizer which learn the dictionay from only X.\n",
    "\n",
    "Prediction step normally does not involve fitting, and it applies to a fitted transfrom. In this case pipeline.predict(X) does est.predict(tf2.transform(tf1.transform(X))), i.e., it applies the transforms to X (without fitting) and the calls est.predict on the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator API summary:\n",
    "* est.fit(X, y): fit estimator to data X, y. Return the fitted estimator (internal parameters are fitted).\n",
    "* est.transform(X): transforms X to another matrix (this matrix is not necessarily similar to y, e.g., in CountVectorizer)\n",
    "* est.fit_transform(X, y) = est.fit(X, y).transform(X)\n",
    "* est.predict(X, y): transform X to something similar to y!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape, iris.target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "\n",
    "clf.score(X_test, y_test)                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Side note: An example implementation of train_test_split\n",
    "# Note: The orig implementation uses iterators and generators. \n",
    "#   Maybe better to use those.\n",
    "import numpy as np\n",
    "def train_test_split(*arrays, **options):\n",
    "        test_size = options.pop('test_size', None)\n",
    "        if test_size is None:\n",
    "            test_size = .1\n",
    "        if len(options) != 0:\n",
    "            raise TypeError(\"Invalid parameters passed: %s\" % str(options))\n",
    "        if len(arrays) == 0:\n",
    "            raise ValueError(\"At least one array required as input\")\n",
    "        \n",
    "        arrays_ = []\n",
    "        m = arrays[0].shape[0]\n",
    "        for a in arrays:\n",
    "            if a.shape[0] != m:\n",
    "                raise ValueError(\"All arrays must have same first dimension\")\n",
    "            if len(a.shape) == 1:\n",
    "                arrays_.append(np.atleast_2d(a).T)\n",
    "            else:\n",
    "                arrays_.append(a)\n",
    "                \n",
    "        p = np.random.permutation(m)\n",
    "        test_ind_max = int(test_size * m)\n",
    "        test_ind = p[:test_ind_max]\n",
    "        train_ind = p[test_ind_max:]\n",
    "        l = []\n",
    "        for a in arrays_:\n",
    "            # Note: a[r1:r2] makes a copy, so modifying the output of this function\n",
    "            # does not affect the original arrays\n",
    "            l.extend([a[train_ind, :], a[test_ind, :]])\n",
    "        return l\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[i for i in train_test_split(iris.data, iris.target)]\n",
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "x1 = [i for i in train_test_split(x, test_size = .5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.\n",
    "\n",
    "The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96666667,  1.        ,  0.96666667,  0.96666667,  1.        ])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "scores                                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use other cross validation strategies by passing a cross validation **iterable** instead, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97777778,  0.97777778,  1.        ,  0.95555556,  1.        ,\n",
       "        0.97777778,  0.97777778,  1.        ,  0.97777778,  0.97777778])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "n_samples = iris.data.shape[0]\n",
    "cv = ShuffleSplit(n_samples, test_size=0.3, random_state=0)\n",
    "cross_val_score(clf, iris.data, iris.target, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97333333333333338"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import metrics\n",
    "predicted = cross_val_predict(clf, iris.data, iris.target, cv=10)\n",
    "print metrics.accuracy_score(iris.target, predicted) \n",
    "np.mean(cross_val_score(clf, iris.data, iris.target, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation Iterable:**\n",
    "* KFold\n",
    "* LeaveOneOut\n",
    "* ShuffleSplit (in later versions)\n",
    "* StratifiedKFold\n",
    "* GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyper-parameters of an estimator\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n",
    "\n",
    "\n",
    "Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, use: estimator.get_params()\n",
    "\n",
    "A search consists of:\n",
    "\n",
    "* an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "* a parameter space;\n",
    "* a method for searching or sampling candidates;\n",
    "* a cross-validation scheme; and\n",
    "* a score function.\n",
    "\n",
    "Two generic approaches to sampling search candidates are provided in scikit-learn: for given values, **GridSearchCV** exhaustively considers all parameter combinations, while **RandomizedSearchCV** can sample a given number of candidates from a parameter space with a specified distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import pandas as pd\n",
    "iris = datasets.load_iris()\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[.1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-3baf1fda6ba0>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-3baf1fda6ba0>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    self.best_estimator_ =\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class GridSearchCV_(estimator, params_grid):\n",
    "    \n",
    "    def fit(X, y):\n",
    "    \n",
    "        grid_search_dims = [len(v) for v in param_grid.values]\n",
    "        res = np.zeros(*grid_search_dims)\n",
    "        indexes, _ = np.ndenumerate(res)\n",
    "        cv = KFold(X.shape[0])\n",
    "        for ind in indexes:\n",
    "            res[ind] = cross_val_score(estimator, X, y, cv)\n",
    "            \n",
    "        best_index = np.unravel_index(np.argmax(res), res.shape)\n",
    "        self.best_params_ = {}\n",
    "        for i, (key, val) in enumerate(paramas.items()):\n",
    "            self.best_params_[key] = val[i]\n",
    "        \n",
    "        self.best_estimator_ = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV and RandomizedSearchCV evaluate each parameter setting independently. Computations can be run in parallel if your OS supports it, by using the keyword n_jobs=-1. See function signature for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimators objects:** http://scikit-learn.org/stable/tutorial/statistical_inference/settings.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def classify(clfs, df, targetname, n_folds, features = None):\n",
    "    if features is None:\n",
    "        features = [c for c in df.columns if c != targetname]\n",
    "    X = df[features].values\n",
    "    y = df[targetname].values\n",
    "    X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    cv = StratifiedShuffleSplit(n_folds)\n",
    "    for clf, parameters in clfs:\n",
    "        fitted_clf = train_classifier(clf, X_tr, y_tr, n_folds, parameters)\n",
    "        print \"test score: %f\" % fitted_clf.score(X_test, y_test)\n",
    "        print\n",
    "        plt = plot_learning_curve(fitted_clf, str(fitted_clf.__class__), X, y, (0.1, 1.01), cv = cv, n_jobs=1)\n",
    "    plt.show()\n",
    "        #print fitted_clt.predict(X_test)\n",
    "        \n",
    "    \n",
    "def train_classifier(clf, X_tr, y_tr, n_folds, parameters):\n",
    "    gs = GridSearchCV(clf, parameters, cv = KFold(X_tr.shape[0], n_folds = n_folds))\n",
    "    print \"=========================================\"\n",
    "    print \"fitting classifier %s\" % clf.__class__\n",
    "    gs.fit(X_tr, y_tr)\n",
    "    print \"best parameters: %s\" % gs.best_params_\n",
    "    return gs.best_estimator_\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(iris.data)\n",
    "df['target'] = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v, t = make_hastie_10_2()\n",
    "df = pd.DataFrame(v)\n",
    "df['target'] = (t == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "fitting classifier <class 'sklearn.svm.classes.SVC'>\n",
      "best parameters: {'C': 10.0, 'gamma': 0.0001}\n",
      "test score: 0.956140\n",
      "\n",
      "=========================================\n",
      "fitting classifier <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "best parameters: {'C': 10.0}\n",
      "test score: 0.947368\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "bc = load_breast_cancer()\n",
    "df = pd.DataFrame(bc.data)\n",
    "df['target'] = bc.target\n",
    "\n",
    "\n",
    "svc = SVC()\n",
    "lr = LogisticRegression()\n",
    "clfs = []\n",
    "clfs.append([svc, {'C': np.logspace(-1, 1, 4), 'gamma': np.logspace(-4, -1, 5)}])\n",
    "clfs.append([lr, {'C': np.logspace(-1, 1, 4)}])\n",
    "\n",
    "classify(clfs, df, 'target', n_folds = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "svc = SVC()\n",
    "digits = load_iris()\n",
    "X, y = digits.data, digits.target\n",
    "#y = y.reshape(-1, 1)\n",
    "print X.shape, y.shape\n",
    "\n",
    "plt = plot_learning_curve(svc, str(svc.__class__), X, y, (0.1, 1.01), cv = StratifiedShuffleSplit(), n_jobs=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v, t = make_hastie_10_2()\n",
    "df = pd.DataFrame(v)\n",
    "df['target'] = (t == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 0.1}\n",
      "test score: 0.533750\n",
      "[0 1 1 ..., 0 1 1]\n",
      "[1 0 1 ..., 1 0 1]\n",
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print classify(clf, df, 'target', parameters= {'C': [.1, 1, 10]}, n_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2,\n",
       "       2, 2, 2, 1, 1, 2, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(df.iloc[120:, :].values[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target[120:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
